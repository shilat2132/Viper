Lexical Analysis:

Implement a lexer to tokenize the input code. 
This involves breaking down the code into meaningful tokens such as:
     keywords, operators, identifiers, and literals.

     keywords - if, else, for, while, return, nope(None), else-if
     operators - &, |, +, -, *, //, %, ==, !, !=, <, >, = (assignment)
     identifiers - name of variables/functions
     literals - fixed values for vars:
          -Integer Literals: 42, 1000
          -Floating-Point Literals: 3.14, 0.001
          -String Literals: "Hello, World!", 'Python'
          -Boolean Literals: True, False


TODO:
- learn regular expressions
- build regular expression for each token format
- build a class token. fields: 
     type: type of token (keyword, operator,...)
     value: the given word in thr code
     position: placement in the source code

     Example: Token(type='NUMBER', value=42, position=(1, 5)) # literal of a number 42 in line 1, word 5
               Token(type='IDENTIFIER', value='x', position=(1, 8))

- lexer accept whole source code as a string, tokenize it and saves all tokens in a literals
- assuming the whole proccess is in one class, it would have a field of tokens