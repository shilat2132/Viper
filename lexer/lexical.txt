Lexical Analysis:

Implement a lexer to tokenize the input code. 
This involves breaking down the code into meaningful tokens such as:
     keywords, operators, identifiers, and literals.

     keywords - if, else, for, while, return, nope(None), else-if
     operators - &, |, +, -, *, //, %, ==, !, !=, <, >, = (assignment)
     identifiers - name of variables/functions
     literals - fixed values for vars:
          -Integer Literals: 42, 1000
          -Floating-Point Literals: 3.14, 0.001
          -String Literals: "Hello, World!", 'Python'
          -Boolean Literals: True, False


TODO:
- learn regular expressions
- build regular expression for each token format
- build a class token. fields: 
     type: type of token (keyword, operator,...)
     value: the given word in the code
     position: placement in the source code

     Example: Token(type='NUMBER', value=42, position=(1, 5)) # literal of a number 42 in line 1, word 5
               Token(type='IDENTIFIER', value='x', position=(1, 8))

- lexer accept whole source code as a string, tokenize it and saves all tokens in a literals
- assuming the whole proccess is in one class, it would have a field of tokens


keywords: while|for|if|else|print, function call, function definition
operators: logical: ==, !=, &&, || !
           arithmetic:=, +, *, /, %, -, ^
identifiers: variablesNames
literal: Number, string, Boolean
Parenthesis = () for function call, {} for a block, [] for array, () for tuples


MISSING:
V*not as !
V*return, none - keywords
*[] for array, () for tuples - literals. distinguish between () of tuples and of condition or expression. 
     these literals won't necessarily have commas in them because they might have only one element